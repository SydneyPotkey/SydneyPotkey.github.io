{
  "hash": "f74b7b6b986e11e4de432f6a88fec57e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 8\"\ndescription: \"Web Scraping\"\nauthor:\n  - name: Sydney Potkey\n    url: https://SydneyPotkey.github.io/\ndate: 06-07-2025\ncitation: \n  url: https://SydneyPotkey.github.io/Posts/06_04_25_Lab8/ \nimage: \"images/download.jpg\"\ndraft: false \n---\n\n> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset\n> of characteristics about different cheeses, and gain deeper insight into your\n> coding process. ðŸª¤\n\n**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize\nwhat you learn from it.\n\n- The only lines on 'https://www.cheese.com/robots.txt' is User-agent: * and Sitemap: https://www.cheese.com/sitemap.xml .\n\n- 'User-agent: *' informs us that anyone is allowed to scrape.\n\n- The lack of Crawl-delay, Visit-time and Request-rate means that we can scrape as often as we want, at any time of day, from as many different users as we want.\n\n- The lack of a 'Disallow\" section means that there are no scraping restrictions on specific areas of the cheese website.\n\n\n**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how\nthis function works with a small example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.0.4     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter()         masks stats::filter()\nâœ– readr::guess_encoding() masks rvest::guess_encoding()\nâœ– dplyr::lag()            masks stats::lag()\nâ„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(purrr)\n```\n:::\n\n\n- The html_attr() function obtains the value of an attribute. It is frequently used after html_elements on a website. \n\n- Below, html_elements(\"a\") extracts all elements inside the <a> attribute. html_attr(\"href\") extracts the value of href inside the <a> element. In this case the href is the link to the wikipedia article on cats.\n\n- Example below can be found [here](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <p><a href='https://en.wikipedia.org/wiki/Cat'>cats</a></p>\n  <img src='https://cataas.com/cat' width='100' height='200'>\n\")\n\nhtml %>% \n  html_elements(\"a\") %>% \n  html_attr(\"href\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"https://en.wikipedia.org/wiki/Cat\"\n```\n\n\n:::\n:::\n\n\n\n**Part 3:** (Do this alongside Part 4 below.) I \nused [ChatGPT](https://chat.openai.com/chat) to start the process of scraping\ncheese information with the following prompt:\n\n> Write R code using the rvest package that allows me to scrape cheese\n> information from cheese.com.\n\nFully document your process of checking this code. Record any observations you\nmake about where ChatGPT is useful / not useful.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(rvest)\nlibrary(dplyr)\n\n# Define the URL\nurl <- \"https://www.cheese.com/alphabetical\"\n\n# Read the HTML content from the webpage\nwebpage <- read_html(url)\n\n# Extract the cheese names and URLs\ncheese_data <- webpage %>%\n  html_nodes(\".cheese-item\") %>%\n  html_nodes(\"a\") %>%\n  html_attr(\"href\") %>%\n  paste0(\"https://cheese.com\", .)\n\ncheese_names <- webpage %>%\n  html_nodes(\".cheese-item h3\") %>%\n  html_text()\n\n# Create a data frame to store the results\ncheese_df <- data.frame(Name = cheese_names,\n                        URL = cheese_data,\n                        stringsAsFactors = FALSE)\n\n# Print the data frame\nprint(cheese_df)\n```\n:::\n\n\n**Part 4:** Obtain the following information for **all** cheeses in the\ndatabase:\n\n-   cheese name\n-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)\n-   whether or not the cheese has a picture (e.g., \n[gouda](https://www.cheese.com/gouda/) has a picture, but \n[bianco](https://www.cheese.com/bianco/) does not).\n\nTo be kind to the website owners, please add a 1 second pause between page\nqueries. (Note that you can view 100 cheeses at a time.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://www.cheese.com/alphabetical\"\n\nextract_cheese_data <- function(url) {\n  Sys.sleep(1)  \n  page <- read_html(url)\n\ncheese_names <- page %>%\n    html_elements(\"h3 a\") %>%\n    html_text()\n\ncheese_urls <- page %>%\n  html_elements(\"h3 a\") %>%\n  html_attr(\"href\") %>%\n  paste0(\"https://www.cheese.com\", .)\n\ncheese_images <- page %>% \n  html_nodes(\".product-item img\") %>% \n  html_attr(\"class\") %>%\n  (\\(x) x == \"image-exists\")()\n\ntibble(\n    name = cheese_names,\n    url = cheese_urls,\n    image = cheese_images)\n}\n\n\nextract_cheese_data(url)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 Ã— 3\n   name                                url                                 image\n   <chr>                               <chr>                               <lgl>\n 1 \"2 Year Aged Cumin Gouda\"           https://www.cheese.com/2-year-agedâ€¦ TRUE \n 2 \"3-Cheese Italian Blend\"            https://www.cheese.com/3-cheese-itâ€¦ FALSE\n 3 \"30 Month Aged Parmigiano Reggiano\" https://www.cheese.com/30-month-agâ€¦ TRUE \n 4 \"3yrs Aged Vintage Gouda\"           https://www.cheese.com/3yrs-aged-vâ€¦ TRUE \n 5 \"Aarewasser\"                        https://www.cheese.com/aarewasser/  TRUE \n 6 \"Abbaye de Belloc\"                  https://www.cheese.com/abbaye-de-bâ€¦ TRUE \n 7 \"Abbaye de Belval\"                  https://www.cheese.com/abbaye-de-bâ€¦ FALSE\n 8 \"Abbaye de Citeaux\"                 https://www.cheese.com/abbaye-de-câ€¦ TRUE \n 9 \"Abbaye de TamiÃ©\"                   https://www.cheese.com/tamie/       TRUE \n10 \"Abbaye de Timadeuc\"                https://www.cheese.com/abbaye-de-tâ€¦ TRUE \n11 \"Abbaye du Mont des Cats\"           https://www.cheese.com/abbaye-du-mâ€¦ TRUE \n12 \"Abbotâ€™s Gold\"                      https://www.cheese.com/abbots-gold/ FALSE\n13 \"Abertam\"                           https://www.cheese.com/abertam/     FALSE\n14 \"Abondance\"                         https://www.cheese.com/abondance/   TRUE \n15 \"Acapella\"                          https://www.cheese.com/acapella/    FALSE\n16 \"Accasciato \"                       https://www.cheese.com/accasciato/  FALSE\n17 \"Ackawi\"                            https://www.cheese.com/ackawi/      TRUE \n18 \"Acorn\"                             https://www.cheese.com/acorn/       FALSE\n19 \"Adelost\"                           https://www.cheese.com/adelost/     TRUE \n20 \"ADL Brick Cheese\"                  https://www.cheese.com/adl-brick-câ€¦ FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\ncheese_df <- map_dfr(url, extract_cheese_data)\n```\n:::\n\n\n\n**Part 5:** When you go to a particular cheese's page (like \n[gouda](https://www.cheese.com/gouda/)), you'll see more detailed information\nabout the cheese. For [**just 10**]{.underline} of the cheeses in the database,\nobtain the following detailed information:\n\n-   milk information\n-   country of origin\n-   family\n-   type\n-   flavour\n\n(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause\nbetween page queries.)\n\n**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To\nwhat extent do your function(s) adhere to the **principles for writing good functions**?\nTo what extent are your **functions efficient**? To what extent is your \n**iteration of these functions efficient**? \n\n\nEfficiency: \nTo increase efficiency in our scrape_cheese function we initiated vectors to hold the elements of our final data set. We knew we would need these eventually so allocating vectors to them vectorizes our function. We also used function like across() in our get_cheese_info function to avoid writing a more complex function and ensuring that we get the same result each time the function runs. \n\nLimitations in Iteration:\nWe did use a for loop in our scrape_cheese function to iterate through pages on the cheese website. For loops are not optimally efficient in R so reformatting the code to use a map function or apply function could further increase our efficiency. \n\nPrinciples of Writing Good Functions: \nWe used the data.frame function in both functions that we wrote to ensure that output is of the same form each time the function runs. We also gave each element in our functions practical yet descriptive names that should tell anyone familiar with R what is happening in the body of our code and what the end goal should contain. Our functions are also self contained not relying on any information from outside the function. \n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}